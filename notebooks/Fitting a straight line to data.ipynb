{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fitting a straight line to data\n",
    "\n",
    "See also: the lecture notes notebook in the same directory\n",
    "\n",
    "How we'll do this, in pseudo-code: \n",
    "\n",
    "```\n",
    "Iterate:\n",
    "- I'll talk for a bit\n",
    "- If we hit an exercise\n",
    "    - split into groups of 3 to solve it (5 minutes per exercise)\n",
    "    - one group will come up and implement on my computer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Python imports we'll need later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "plt.style.use('apw-notebook.mplstyle')\n",
    "%matplotlib inline\n",
    "rnd = np.random.RandomState(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y = a\\,x + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_data = 16 # number of data points\n",
    "a_true = 1.255 # randomly chosen truth\n",
    "b_true = 4.507 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# randomly generate some x values over some domain by sampling from a uniform distribution\n",
    "x = rnd.uniform(0, 2., n_data)\n",
    "x.sort() # sort the values in place\n",
    "\n",
    "# evaluate the true model at the given x values\n",
    "y = a_true*x + b_true\n",
    "\n",
    "# Heteroscedastic Gaussian uncertainties only in y direction\n",
    "y_err = rnd.uniform(0.1, 0.2, size=n_data) # randomly generate uncertainty for each datum\n",
    "y = rnd.normal(y, y_err) # re-sample y data with noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, y_err, linestyle='none', marker='o', ecolor='#666666')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now forget everything we just did! Someone just handed these data to you and we want to fit a model.\n",
    "\n",
    "### Exercise 1: \n",
    "\n",
    "Implement the functions to compute the weighted deviations below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def line_model(pars, x):\n",
    "    \"\"\"\n",
    "    Evaluate a straight line model at the input x values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pars : list, array\n",
    "        This should be a length-2 array or list containing the \n",
    "        parameter values (a, b) for the (slope, intercept).\n",
    "    x : numeric, list, array\n",
    "        The coordinate values.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    y : array\n",
    "        The computed y values at each input x.\n",
    "    \"\"\"\n",
    "    return pars[0]*np.array(x) + pars[1]\n",
    "\n",
    "def weighted_absolute_deviation(pars, x, y, y_err):\n",
    "    \"\"\"\n",
    "    Compute the weighted absolute deviation between the data \n",
    "    (x, y, y_err) and the model points computed with the input \n",
    "    parameters (pars).\n",
    "    \"\"\"\n",
    "    \n",
    "    # IMPLEMENT ME\n",
    "    \n",
    "    pass\n",
    "\n",
    "def weighted_squared_deviation(pars, x, y, y_err):\n",
    "    \"\"\"\n",
    "    Compute the weighted squared deviation between the data \n",
    "    (x, y, y_err) and the model points computed with the input \n",
    "    parameters (pars).\n",
    "    \"\"\"\n",
    "    \n",
    "    # IMPLEMENT ME\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a 256x256 grid of parameter values centered on the true values\n",
    "a_grid = np.linspace(a_true-2., a_true+2, 256)\n",
    "b_grid = np.linspace(b_true-2., b_true+2, 256)\n",
    "a_grid,b_grid = np.meshgrid(a_grid, b_grid)\n",
    "ab_grid = np.vstack((a_grid.ravel(), b_grid.ravel())).T\n",
    "\n",
    "# a reshaped 256x256 grid of parameter values:\n",
    "ab_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1, 2, figsize=(9,5.1), sharex=True, sharey=True)\n",
    "\n",
    "for i,func in enumerate([weighted_absolute_deviation, weighted_squared_deviation]):\n",
    "    func_vals = np.zeros(ab_grid.shape[0])\n",
    "    for j,pars in enumerate(ab_grid):\n",
    "        func_vals[j] = func(pars, x, y, y_err)\n",
    "\n",
    "    axes[i].pcolormesh(a_grid, b_grid, func_vals.reshape(a_grid.shape), \n",
    "                       cmap='Blues', vmin=func_vals.min(), vmax=func_vals.min()+256) # arbitrary scale\n",
    "    \n",
    "    axes[i].set_xlabel('$a$')\n",
    "    \n",
    "    # plot the truth\n",
    "    axes[i].plot(a_true, b_true, marker='o', zorder=10, color='#de2d26')\n",
    "    axes[i].axis('tight')\n",
    "    axes[i].set_title(func.__name__, fontsize=14)\n",
    "\n",
    "axes[0].set_ylabel('$b$')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use one of the numerical function minimizers from Scipy to minimize these two functions and compare the resulting \"fits\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x0 = [1., 1.] # starting guess for the optimizer \n",
    "\n",
    "result_abs = minimize(weighted_absolute_deviation, x0=x0, \n",
    "                      args=(x, y, y_err), # passed to the weighted_*_deviation function after pars \n",
    "                      method='BFGS') # similar to Newton's method\n",
    "\n",
    "result_sq = minimize(weighted_squared_deviation, x0=x0, \n",
    "                     args=(x, y, y_err), # passed to the weighted_*_deviation function after pars\n",
    "                     method='BFGS')\n",
    "\n",
    "best_pars_abs = result_abs.x\n",
    "best_pars_sq = result_sq.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot our two best-fit lines over the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, y_err, linestyle='none', marker='o', ecolor='#666666')\n",
    "\n",
    "x_grid = np.linspace(x.min()-0.1, x.max()+0.1, 128)\n",
    "plt.plot(x_grid, line_model(best_pars_abs, x_grid), \n",
    "         marker='', linestyle='-', label='absolute deviation')\n",
    "plt.plot(x_grid, line_model(best_pars_sq, x_grid), \n",
    "         marker='', linestyle='-', label='squared deviation')\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least-squares / maximum likelihood with matrix calculus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\trpo}[1]{{#1}^{\\mathsf{T}}}\n",
    "\\newcommand{\\bs}[1]{\\boldsymbol{#1}}\n",
    "\\bs{\\theta}_{\\rm best} = \\left[\\trpo{\\bs{X}} \\, \\bs{\\Sigma}^{-1} \\, \\bs{X}\\right]^{-1} \\, \n",
    "    \\trpo{\\bs{X}} \\, \\bs{\\Sigma}^{-1} \\, \\bs{y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\newcommand{\\trpo}[1]{{#1}^{\\mathsf{T}}}\n",
    "\\newcommand{\\bs}[1]{\\boldsymbol{#1}}\n",
    "C = \\left[\\trpo{\\bs{X}} \\, \\bs{\\Sigma}^{-1} \\, \\bs{X}\\right]^{-1}\n",
    "$$\n",
    "\n",
    "### Exercise 2: \n",
    "\n",
    "Implement the necessary linear algebra to solve for the best-fit parameters and the parameter covariance matrix, defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create matrices and vectors:\n",
    "\n",
    "# Define the design matrix, X:\n",
    "# X = \n",
    "\n",
    "# Define the data covariance matrix, Cov:\n",
    "# Cov = \n",
    "\n",
    "Cinv = np.linalg.inv(Cov) # we'll need the inverse covariance matrix below\n",
    "X.shape, Cov.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write the matrix operations to get the parameter covariance matrix. It \n",
    "# might help to know that you can transpose a numpy array with .T and that \n",
    "# you can multiply two matrices or a matrix and a vector with the \n",
    "# new @ operator (Python >=3.5):\n",
    "# pars_Cov = \n",
    "\n",
    "# Write out the necessary matrix operations to get the optimal parameters. \n",
    "# Use the pars_Cov object from above: \n",
    "# best_pars_linalg = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_pars_sq - best_pars_linalg[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll plot the 1 and 2-sigma error ellipses using the parameter covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some tricks to get info we need to plot an ellipse, aligned with \n",
    "#    the eigenvectors of the covariance matrix\n",
    "eigval,eigvec = np.linalg.eig(pars_Cov)\n",
    "angle = np.degrees(np.arctan2(eigvec[1,0], eigvec[0,0]))\n",
    "w,h = 2*np.sqrt(eigval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "fig,ax = plt.subplots(1, 1, figsize=(5,5))\n",
    "\n",
    "for n in [1,2]:\n",
    "    ax.add_patch(Ellipse(best_pars_linalg, width=n*w, height=n*h, angle=angle, \n",
    "                         fill=False, linewidth=3-n, edgecolor='#555555', \n",
    "                         label=r'{}$\\sigma$'.format(n)))\n",
    "\n",
    "ax.plot(b_true, a_true, marker='o', zorder=10, linestyle='none',\n",
    "        color='#de2d26', label='truth')\n",
    "\n",
    "ax.set_xlabel('$b$')\n",
    "ax.set_ylabel('$a$')\n",
    "ax.legend(loc='best')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bayesian approach\n",
    "\n",
    "Recall that:\n",
    "\n",
    "$$\n",
    "\\ln\\mathcal{L} = -\\frac{1}{2}\\left[N\\,\\ln(2\\pi) \n",
    "    + \\ln|\\boldsymbol{\\Sigma}|\n",
    "    + \\left(\\boldsymbol{y} - \\boldsymbol{X}\\,\\boldsymbol{\\theta}\\right)^\\mathsf{T} \\, \n",
    "      \\boldsymbol{\\Sigma}^{-1} \\,\n",
    "      \\left(\\boldsymbol{y} - \\boldsymbol{X}\\,\\boldsymbol{\\theta}\\right)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "but, with our assumptions, $\\boldsymbol{\\Sigma}$ is diagonal. We can replace the matrix operations with sums.\n",
    "\n",
    "### Exercise 3:\n",
    "\n",
    "Implement the log-prior method (`ln_prior`) on the model class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StraightLineModel(object):\n",
    "    \n",
    "    def __init__(self, x, y, y_err):\n",
    "        \"\"\" \n",
    "        We store the data as attributes of the object so we don't have to \n",
    "        keep passing it in to the methods that compute the probabilities.\n",
    "        \"\"\"\n",
    "        self.x = np.asarray(x)\n",
    "        self.y = np.asarray(y)\n",
    "        self.y_err = np.asarray(y_err)\n",
    "\n",
    "    def ln_likelihood(self, pars):\n",
    "        \"\"\"\n",
    "        We don't need to pass in the data because we can access it from the\n",
    "        attributes. This is basically the same as the weighted squared \n",
    "        deviation function, but includes the constant normalizations for the\n",
    "        Gaussian likelihood.\n",
    "        \"\"\"\n",
    "        N = len(y)\n",
    "        dy = y - line_model(pars, self.x)\n",
    "        ivar = 1 / self.y_err**2 # inverse-variance\n",
    "        return -0.5 * (N*np.log(2*np.pi) + np.sum(2*np.log(self.y_err)) + np.sum(dy**2 * ivar))\n",
    "\n",
    "    def ln_prior(self, pars):\n",
    "        \"\"\" \n",
    "        The prior only depends on the parameters, so we don't need to touch\n",
    "        the data at all. We're going to implement a flat (uniform) prior \n",
    "        over the ranges:\n",
    "            a : [0, 100]\n",
    "            b : [-50, 50]\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        a, b = pars # unpack parameters\n",
    "        ln_prior_val = 0. # we'll add to this\n",
    "    \n",
    "        # IMPLEMENT ME\n",
    "        # if a is inside the range above, add log(1/100) to ln_prior_val, otherwise return -infinity\n",
    "        \n",
    "        # IMPLEMENT ME\n",
    "        # if b is inside the range above, add log(1/100) to ln_prior_val, otherwise return -infinity\n",
    "\n",
    "        return ln_prior_val\n",
    "\n",
    "    def ln_posterior(self, pars):\n",
    "        \"\"\" \n",
    "        Up to a normalization constant, the log of the posterior pdf is just \n",
    "        the sum of the log likelihood plus the log prior.\n",
    "        \"\"\"\n",
    "        lnp = self.ln_prior(pars)\n",
    "        if np.isinf(lnp): # short-circuit if the prior is infinite (don't bother computing likelihood)\n",
    "            return lnp\n",
    "\n",
    "        lnL = self.ln_likelihood(pars)\n",
    "        lnprob = lnp + lnL\n",
    "\n",
    "        if np.isnan(lnprob):\n",
    "            return -np.inf\n",
    "\n",
    "        return lnprob\n",
    "    \n",
    "    def __call__(self, pars):\n",
    "        return self.ln_posterior(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate the model object with the data\n",
    "model = StraightLineModel(x, y, y_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we'll repeat what we did above to map out the value of the log-posterior over a 2D grid of parameter values. Because we used a flat prior, you'll notice it looks identical to the visualization of the `weighted_squared_deviation` -- only the likelihood has any slope to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_on_grid(func, a_grid, b_grid, args=()):\n",
    "    a_grid,b_grid = np.meshgrid(a_grid, b_grid)\n",
    "    ab_grid = np.vstack((a_grid.ravel(), b_grid.ravel())).T\n",
    "    \n",
    "    func_vals = np.zeros(ab_grid.shape[0])\n",
    "    for j,pars in enumerate(ab_grid):\n",
    "        func_vals[j] = func(pars, *args)\n",
    "        \n",
    "    return func_vals.reshape(a_grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1, 3, figsize=(14,5.1), sharex=True, sharey=True)\n",
    "\n",
    "# make a 256x256 grid of parameter values centered on the true values\n",
    "a_grid = np.linspace(a_true-5., a_true+5, 256)\n",
    "b_grid = np.linspace(b_true-5., b_true+5, 256)\n",
    "\n",
    "ln_prior_vals = evaluate_on_grid(model.ln_prior, a_grid, b_grid)\n",
    "ln_like_vals = evaluate_on_grid(model.ln_likelihood, a_grid, b_grid)\n",
    "ln_post_vals = evaluate_on_grid(model.ln_posterior, a_grid, b_grid)\n",
    "\n",
    "for i,vals in enumerate([ln_prior_vals, ln_like_vals, ln_post_vals]):\n",
    "    axes[i].pcolormesh(a_grid, b_grid, vals, \n",
    "                       cmap='Blues', vmin=vals.max()-1024, vmax=vals.max()) # arbitrary scale\n",
    "    \n",
    "axes[0].set_title('log-prior', fontsize=20)\n",
    "axes[1].set_title('log-likelihood', fontsize=20)\n",
    "axes[2].set_title('log-posterior', fontsize=20)\n",
    "    \n",
    "for ax in axes:\n",
    "    ax.set_xlabel('$a$')\n",
    "    \n",
    "    # plot the truth\n",
    "    ax.plot(a_true, b_true, marker='o', zorder=10, color='#de2d26')\n",
    "    ax.axis('tight')\n",
    "\n",
    "axes[0].set_ylabel('$b$')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: \n",
    "\n",
    "Subclass the `StraightLineModel` class and implement a new prior. Replace the flat prior above with an uncorrelated 2D Gaussian centered on $(\\mu_a,\\mu_b) = (3., 5.5)$ with root-variances $(\\sigma_a,\\sigma_b) = (0.05, 0.05)$. Compare the 2D grid plot with the flat prior to the one with a Gaussian prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StraightLineModelGaussianPrior(StraightLineModel): # verbose names are a good thing!\n",
    "    \n",
    "    def ln_prior(self, pars):\n",
    "        a, b = pars # unpack parameters\n",
    "        ln_prior_val = 0. # we'll add to this\n",
    "    \n",
    "        # IMPLEMENT ME\n",
    "        # prior on a is a Gaussian with mean, stddev = (3, 0.05)\n",
    "        \n",
    "        # IMPLEMENT ME\n",
    "        # prior on b is a Gaussian with mean, stddev = (5.5, 0.05)\n",
    "\n",
    "        return ln_prior_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_Gprior = StraightLineModelGaussianPrior(x, y, y_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1, 3, figsize=(14,5.1), sharex=True, sharey=True)\n",
    "\n",
    "ln_prior_vals2 = evaluate_on_grid(model_Gprior.ln_prior, a_grid, b_grid)\n",
    "ln_like_vals2 = evaluate_on_grid(model_Gprior.ln_likelihood, a_grid, b_grid)\n",
    "ln_post_vals2 = evaluate_on_grid(model_Gprior.ln_posterior, a_grid, b_grid)\n",
    "\n",
    "for i,vals in enumerate([ln_prior_vals2, ln_like_vals2, ln_post_vals2]):\n",
    "    axes[i].pcolormesh(a_grid, b_grid, vals, \n",
    "                       cmap='Blues', vmin=vals.max()-1024, vmax=vals.max()) # arbitrary scale\n",
    "    \n",
    "axes[0].set_title('log-prior', fontsize=20)\n",
    "axes[1].set_title('log-likelihood', fontsize=20)\n",
    "axes[2].set_title('log-posterior', fontsize=20)\n",
    "    \n",
    "for ax in axes:\n",
    "    ax.set_xlabel('$a$')\n",
    "    \n",
    "    # plot the truth\n",
    "    ax.plot(a_true, b_true, marker='o', zorder=10, color='#de2d26')\n",
    "    ax.axis('tight')\n",
    "\n",
    "axes[0].set_ylabel('$b$')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Well now switch back to using the uniform / flat prior.\n",
    "\n",
    "---\n",
    "\n",
    "## MCMC\n",
    "\n",
    "The simplest MCMC algorithm is \"Metropolis-Hastings\". I'm not going to explain it in detail, but in pseudocode, it looks like this:\n",
    "\n",
    "- Start from some position in parameter space, $\\theta_0$ with posterior probability $\\pi_0$\n",
    "- Iterate from 1 to $N_{\\rm steps}$:\n",
    "    - Sample an offset from $\\delta\\theta_0$ from some proposal distribution\n",
    "    - Compute a new parameter value using this offset, $\\theta_{\\rm new} = \\theta_0 + \\delta\\theta_0$\n",
    "    - Evaluate the posterior probability at the new new parameter vector, $\\pi_{\\rm new}$\n",
    "    - Sample a uniform random number, $r \\sim \\mathcal{U}(0,1)$\n",
    "    - if $\\pi_{\\rm new}/\\pi_0 > 1$ or $\\pi_{\\rm new}/\\pi_0 > r$:\n",
    "        - store $\\theta_{\\rm new}$\n",
    "        - replace $\\theta_0,\\pi_0$ with $\\theta_{\\rm new},\\pi_{\\rm new}$\n",
    "    - else:\n",
    "        - store $\\theta_0$ again\n",
    "        \n",
    "The proposal distribution has to be chosen and tuned by hand. We'll use a spherical / uncorrelated Gaussian distribution with root-variances set by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_proposal(*sigmas):\n",
    "    return np.random.normal(0., sigmas)\n",
    "\n",
    "def run_metropolis_hastings(p0, n_steps, model, proposal_sigmas):\n",
    "    \"\"\"\n",
    "    Run a Metropolis-Hastings MCMC sampler to generate samples from the input\n",
    "    log-posterior function, starting from some initial parameter vector.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p0 : iterable\n",
    "        Initial parameter vector.\n",
    "    n_steps : int\n",
    "        Number of steps to run the sampler for.\n",
    "    model : StraightLineModel instance (or subclass)\n",
    "        A callable object that takes a parameter vector and computes \n",
    "        the log of the posterior pdf.\n",
    "    proposal_sigmas : list, array\n",
    "        A list of standard-deviations passed to the sample_proposal \n",
    "        function. These are like step sizes in each of the parameters.\n",
    "    \"\"\"\n",
    "    p0 = np.array(p0)\n",
    "    if len(proposal_sigmas) != len(p0):\n",
    "        raise ValueError(\"Proposal distribution should have same shape as parameter vector.\")\n",
    "    \n",
    "    # the objects we'll fill and return:\n",
    "    chain = np.zeros((n_steps, len(p0))) # parameter values at each step\n",
    "    ln_probs = np.zeros(n_steps) # log-probability values at each step\n",
    "    \n",
    "    # we'll keep track of how many steps we accept to compute the acceptance fraction                        \n",
    "    n_accept = 0 \n",
    "    \n",
    "    # evaluate the log-posterior at the initial position and store starting position in chain\n",
    "    ln_probs[0] = model(p0)\n",
    "    chain[0] = p0\n",
    "    \n",
    "    # loop through the number of steps requested and run MCMC\n",
    "    for i in range(1,n_steps):\n",
    "        # proposed new parameters\n",
    "        step = sample_proposal(*proposal_sigmas)\n",
    "        new_p = chain[i-1] + step\n",
    "        \n",
    "        # compute log-posterior at new parameter values\n",
    "        new_ln_prob = model(new_p)\n",
    "        \n",
    "        # log of the ratio of the new log-posterior to the previous log-posterior value\n",
    "        ln_prob_ratio = new_ln_prob - ln_probs[i-1]\n",
    "        \n",
    "        if (ln_prob_ratio > 0) or (ln_prob_ratio > np.log(np.random.uniform())):\n",
    "            chain[i] = new_p\n",
    "            ln_probs[i] = new_ln_prob\n",
    "            n_accept += 1\n",
    "            \n",
    "        else:\n",
    "            chain[i] = chain[i-1]\n",
    "            ln_probs[i] = ln_probs[i-1]\n",
    "    \n",
    "    acc_frac = n_accept / n_steps\n",
    "    return chain, ln_probs, acc_frac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:\n",
    "\n",
    "Choose a starting position, values for `a` and `b` to start the MCMC from. In general, a good way to do this is to sample from the prior pdf. Generate values for `a` and `b` by sampling from a uniform distribution over the domain we defined above. Then, run the MCMC sampler from this initial position for 8192 steps. Play around with (\"tune\" as they say) the `proposal_sigmas` until you get an acceptance fraction around ~40%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# starting position:\n",
    "# p0 = \n",
    "\n",
    "# execute run_metropolis_hastings():\n",
    "# chain,probs,acc_frac = run_metropolis_hastings(...)\n",
    "\n",
    "print(\"Acceptance fraction: {:.1%}\".format(acc_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the MCMC chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 1, figsize=(5,5))\n",
    "\n",
    "ax.pcolormesh(a_grid, b_grid, ln_post_vals, # from the grid evaluation way above\n",
    "              cmap='Blues', vmin=vals.max()-1024, vmax=vals.max()) # arbitrary scale\n",
    "ax.axis('tight')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "ax.plot(a_true, b_true, marker='o', zorder=10, color='#de2d26')\n",
    "ax.plot(chain[:512,0], chain[:512,1], marker='', color='k', linewidth=1.)\n",
    "\n",
    "ax.set_xlabel('$a$')\n",
    "ax.set_ylabel('$b$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the individual parameter traces, i.e. the 1D functions of parameter value vs. step number for each parameter separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(len(p0), 1, figsize=(5,7), sharex=True)\n",
    "\n",
    "for i in range(len(p0)):\n",
    "    axes[i].plot(chain[:,i], marker='', drawstyle='steps')\n",
    "    \n",
    "axes[0].axhline(a_true, color='r', label='true')\n",
    "axes[0].legend(loc='best')\n",
    "axes[0].set_ylabel('$a$')\n",
    "\n",
    "axes[1].axhline(b_true, color='r')\n",
    "axes[1].set_ylabel('$b$')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Remove the \"burn-in\" phase and thin the chains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "good_samples = chain[2000::8]\n",
    "good_samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What values should we put in the abstract?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "low,med,hi = np.percentile(good_samples, [16, 50, 84], axis=0)\n",
    "upper, lower = hi-med, med-low\n",
    "\n",
    "disp_str = \"\"\n",
    "for i,name in enumerate(['a', 'b']):\n",
    "    fmt_str = '{name}={val:.2f}^{{+{plus:.2f}}}_{{-{minus:.2f}}}'\n",
    "    disp_str += fmt_str.format(name=name, val=med[i], plus=upper[i], minus=lower[i])\n",
    "    disp_str += r'\\quad '\n",
    "    \n",
    "disp_str = \"${}$\".format(disp_str)\n",
    "display.Latex(data=disp_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the true values are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_true, b_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot lines sampled from the posterior pdf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.errorbar(x, y, y_err, linestyle='none', marker='o', ecolor='#666666')\n",
    "\n",
    "x_grid = np.linspace(x.min()-0.1, x.max()+0.1, 128)\n",
    "for pars in good_samples[:128]: # only plot 128 samples\n",
    "    plt.plot(x_grid, line(pars, x_grid), \n",
    "             marker='', linestyle='-', color='#3182bd', alpha=0.1, zorder=-10)\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can plot the samples using a _corner plot_ to visualize the structure of the 2D and 1D (marginal) posteriors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment and run this line if the import fails:\n",
    "# !source activate statsseminar; pip install corner\n",
    "import corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = corner.corner(chain[2000:], bins=32, labels=['$a$', '$b$'], truths=[a_true, b_true])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Fitting a straight line to data with intrinsic scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V_true = 0.5**2\n",
    "n_data = 42\n",
    "# we'll keep the same parameters for the line as we used above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = rnd.uniform(0, 2., n_data)\n",
    "x.sort() # sort the values in place\n",
    "\n",
    "y = a_true*x + b_true\n",
    "\n",
    "# Heteroscedastic Gaussian uncertainties only in y direction\n",
    "y_err = rnd.uniform(0.1, 0.2, size=n_data) # randomly generate uncertainty for each datum\n",
    "\n",
    "# add Gaussian intrinsic width\n",
    "y = rnd.normal(y, np.sqrt(y_err**2 + V_true)) # re-sample y data with noise and intrinsic scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, y_err, linestyle='none', marker='o', ecolor='#666666')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first naively fit the data assuming no intrinsic scatter using least-squares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.vander(x, N=2, increasing=True) \n",
    "Cov = np.diag(y_err**2)\n",
    "Cinv = np.linalg.inv(Cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_pars = np.linalg.inv(X.T @ Cinv @ X) @ (X.T @ Cinv @ y)\n",
    "pars_Cov = np.linalg.inv(X.T @ Cinv @ X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, y_err, linestyle='none', marker='o', ecolor='#666666')\n",
    "\n",
    "x_grid = np.linspace(x.min()-0.1, x.max()+0.1, 128)\n",
    "plt.plot(x_grid, line(best_pars[::-1], x_grid), marker='', linestyle='-', label='best-fit line')\n",
    "plt.plot(x_grid, line([a_true, b_true], x_grid), marker='', linestyle='-', label='true line')\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance matrix for the parameters is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pars_Cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6:\n",
    "\n",
    "Subclass the `StraightLineModel` class and implement new prior and likelihood functions (`ln_prior` and `ln_likelihood`). The our model will now have 3 parameters: `a`, `b`, and `lnV` the log of the intrinsic scatter variance. Use flat priors on all of these parameters. In fact, we'll be even lazier and forget the constant normalization terms: if a parameter vector is within the ranges below, return 0. (log(1.)) otherwise return -infinity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StraightLineIntrinsicScatterModel(StraightLineModel):\n",
    "\n",
    "    def ln_prior(self, pars): \n",
    "        \"\"\" The prior only depends on the parameters \"\"\"\n",
    "\n",
    "        a, b, lnV = pars\n",
    "\n",
    "        # flat priors on a, b, lnV: same bounds on each, (-100,100)\n",
    "        # IMPLEMENT ME\n",
    "\n",
    "        # this is only valid up to a numerical constant \n",
    "        return 0.\n",
    "    \n",
    "    def ln_likelihood(self, pars):\n",
    "        \"\"\" The likelihood function evaluation requires a particular set of model parameters and the data \"\"\"\n",
    "        a,b,lnV = pars\n",
    "        V = np.exp(lnV)\n",
    "        \n",
    "        # IMPLEMENT ME\n",
    "        # the variance now has to include the intrinsic scatter V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scatter_model = StraightLineIntrinsicScatterModel(x, y, y_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x0 = [5., 5., 0.] # starting guess for the optimizer \n",
    "\n",
    "# we have to minimize the negative log-likelihood to maximize the likelihood\n",
    "result_ml_scatter = minimize(lambda *args: -scatter_model.ln_likelihood(*args), \n",
    "                             x0=x0, method='BFGS')\n",
    "result_ml_scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, y_err, linestyle='none', marker='o', ecolor='#666666')\n",
    "\n",
    "x_grid = np.linspace(x.min()-0.1, x.max()+0.1, 128)\n",
    "plt.plot(x_grid, line(result_ml_scatter.x[:2], x_grid), marker='', linestyle='-', label='best-fit line')\n",
    "plt.plot(x_grid, line([a_true, b_true], x_grid), marker='', linestyle='-', label='true line')\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V_true, np.exp(result_ml_scatter.x[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7:\n",
    "\n",
    "To quantify our uncertainty in the parameters, we'll run MCMC using the new model. Run MCMC for 65536 steps and visualize the resulting chain. Make sure the acceptance fraction is between ~25-50%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# IMPLEMENT ME\n",
    "# p0 = \n",
    "# chain,probs,acc_frac = ... \n",
    "\n",
    "print(\"Acceptance fraction: {:.1%}\".format(acc_frac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# IMPLEMENT ME\n",
    "# plot the 1D parameter traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPLEMENT ME\n",
    "# make a corner plot of the samples after they have converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# IMPLEMENT ME\n",
    "# thin the chain, remove burn-in, and report median and percentiles:\n",
    "# good_samples = \n",
    "\n",
    "low,med,hi = np.percentile(good_samples, [16, 50, 84], axis=0)\n",
    "upper, lower = hi-med, med-low\n",
    "\n",
    "disp_str = \"\"\n",
    "for i,name in enumerate(['a', 'b', r'\\ln V']):\n",
    "    fmt_str = '{name}={val:.2f}^{{+{plus:.2f}}}_{{-{minus:.2f}}}'\n",
    "    disp_str += fmt_str.format(name=name, val=med[i], plus=upper[i], minus=lower[i])\n",
    "    disp_str += r'\\quad '\n",
    "    \n",
    "disp_str = \"${}$\".format(disp_str)\n",
    "display.Latex(data=disp_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to the diagonal elements of the covariance matrix we got from ignoring the intrinsic scatter and doing least-squares fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disp_str = \"\"\n",
    "for i,name in zip([1,0], ['a', 'b']):\n",
    "    fmt_str = r'{name}={val:.2f} \\pm {err:.2f}'\n",
    "    disp_str += fmt_str.format(name=name, val=best_pars[i], err=np.sqrt(pars_Cov[i,i]))\n",
    "    disp_str += r'\\quad '\n",
    "    \n",
    "disp_str = \"${}$\".format(disp_str)\n",
    "display.Latex(data=disp_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the percentiles of the marginal posterior samples as compared to the least-squares parameter uncertainties?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:statsseminar]",
   "language": "python",
   "name": "conda-env-statsseminar-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
